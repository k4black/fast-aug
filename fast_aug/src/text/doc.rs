use std::collections::HashSet;
use super::token::{Token, TokenType};
use unicode_segmentation::UnicodeSegmentation;


/// Doc struct holds content as a list of tokens.
/// TODO: Lazy loading of tokens while not requested.
pub struct Doc {
    pub tokens: Vec<Token>,
    pub num_changes: usize,
}

impl Doc {
    /// Create a new Doc from a string slice.
    /// Automatically tokenizes the text on word boundaries (words, spaces, and special symbols).
    /// See https://www.unicode.org/reports/tr29/#Word_Boundaries
    ///
    /// # Arguments
    /// * `text` - A string slice that holds the text to be tokenized.
    pub fn new(text: &str) -> Self {
        let tokens = Doc::tokenize(text);
        Doc {
            tokens,
            num_changes: 0,
        }
    }

    /// Create a new Doc from a list of tokens.
    /// Select token type automatically.
    ///
    /// # Arguments
    /// * `tokens` - A vector of string slices that holds the tokens.
    pub fn from_tokens(tokens: Vec<&str>) -> Self {
        let tokens = tokens.iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
        Doc {
            tokens,
            num_changes: 0,
        }
    }

    /// Tokenize a string slice on word boundaries (words, spaces, and special symbols).
    /// Use "Unicode Standard Annex #29" https://www.unicode.org/reports/tr29/#Word_Boundaries
    fn tokenize(text: &str) -> Vec<Token> {
        UnicodeSegmentation::split_word_bounds(text)
            .map(Token::from_str)
            .collect()
    }

    pub fn len(&self) -> usize {
        self.tokens.len()
    }

    pub fn is_empty(&self) -> bool {
        self.tokens.is_empty()
    }

    /// Convert Doc to string
    pub fn to_string(&self) -> String {
        self.tokens.iter().map(|token| token.token().as_str()).collect::<Vec<&str>>().join("")
    }

    /// Calculate number of word tokens
    ///
    /// # Arguments
    /// * `include_special_char` - Include Special tokens in count
    pub fn get_word_tokens_count(&self, include_special_char: bool) -> usize {
        let mut count = 0;
        for token in self.tokens.iter() {
            let token_type = token.kind();
            match (token_type, include_special_char) {
                (TokenType::Word, _) => count += 1,
                (TokenType::Special, true) => count += 1,
                (_, _) => (),
            }
        }
        count
    }

    /// Get only WordTokens original indexes
    ///
    /// # Arguments
    /// * `include_special_char` - Include Special tokens in count
    /// * `stopwords` - A HashSet of stopwords to be skipped
    pub fn get_word_indexes(&mut self, include_special_char: bool, stopwords: Option<&HashSet<String>>) -> Vec<usize> {
        let mut word_indexes = Vec::with_capacity(self.tokens.len());

        for (idx, token) in self.tokens.iter().enumerate() {
            let token_type = token.kind();
            match (token_type, include_special_char) {
                (TokenType::Word, _) => {
                    if let Some(stopwords) = stopwords {
                        if stopwords.contains(token.token()) {
                            continue;
                        }
                    }
                    word_indexes.push(idx);
                },
                (TokenType::Special, true) => word_indexes.push(idx),
                (_, _) => (),
            }
        }

        word_indexes
    }

    /// Swap two tokens in Doc - in-place
    ///
    /// # Arguments
    /// * `idx_a` - Index of first token
    /// * `idx_b` - Index of second token
    pub fn swap_tokens_by_index(&mut self, idx_a: usize, idx_b: usize) {
        self.tokens.swap(idx_a, idx_b);
    }
}


#[cfg(test)]
mod tests {
    use test_case::test_case;
    use super::*;

    #[test_case("Hello, world!", vec!["Hello", ",", " ", "world", "!"] ; "basic latin script")]
    #[test_case("    Some\t\t    spaces", vec!["    ", "Some", "\t", "\t", "    ", "spaces"] ; "complicated spaces")]
    #[test_case("Hello\u{200A}world", vec!["Hello", "\u{200A}", "world"] ; "unicode spaces")]
    #[test_case(".!@#$%^&*()_+", vec![".", "!", "@", "#", "$", "%", "^", "&", "*", "(", ")", "_", "+"] ; "special characters")]
    #[test_case("ЁЯШЫЁЯЩИЁЯШЖ", vec!["ЁЯШЫ", "ЁЯЩИ", "ЁЯШЖ"] ; "emoji")]
    #[test_case(":clown_face: :see_no_evil: :laughing:", vec![":", "clown_face", ":", " ", ":", "see_no_evil", ":", " ", ":", "laughing", ":"] ; "emoji aliases")]
    fn test_docs_spesial_cases(input_text: &str, expected_tokens: Vec<&str>) {
        let doc = Doc::new(input_text);
        let expected_tokens = expected_tokens.iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
        assert_eq!(doc.tokens.len(), expected_tokens.len());
        assert_eq!(doc.tokens, expected_tokens);
    }

    // from https://generator.lorem-ipsum.info/
    #[test_case(
        "Lorem ipsum dolor sit amet, soluta commune ponderum mea ad, te qui erant vulputate, ne sed noster verear. No illud abhorreant vel, quo no amet liber tantas.",
        vec!["Lorem", " ", "ipsum", " ", "dolor", " ", "sit", " ", "amet", ",", " ", "soluta", " ", "commune", " ", "ponderum", " ", "mea", " ", "ad", ",", " ", "te", " ", "qui", " ", "erant", " ", "vulputate", ",", " ", "ne", " ", "sed", " ", "noster", " ", "verear", ".", " ", "No", " ", "illud", " ", "abhorreant", " ", "vel", ",", " ", "quo", " ", "no", " ", "amet", " ", "liber", " ", "tantas", "."]
        ; "latin script"
    )]
    #[test_case(
        "╨Ы╨╛╤А╨╡╨╝ ╨╕╨┐╤Б╤Г╨╝ ╨┤╨╛╨╗╨╛╤А ╤Б╨╕╤В ╨░╨╝╨╡╤В, ╨┐╨╡╤А ╤Ж╨╗╨╕╤В╨░ ╨┐╨╛╤Б╤Б╨╕╤В ╨╡╤Е, ╨░╤В ╨╝╤Г╨╜╨╡╤А╨╡ ╤Д╨░╨▒╤Г╨╗╨░╤Б ╨┐╨╡╤В╨╡╨╜╤В╨╕╤Г╨╝ ╤Б╨╕╤В. ╨Ш╤Г╤Б ╤Ж╤Г ╤Ж╨╕╨▒╨╛ ╤Б╨░╨┐╨╡╤А╨╡╤В ╤Б╤Ж╤А╨╕╨┐╤Б╨╡╤А╨╕╤В, ╨╜╨╡╤Ж ╨▓╨╕╤Б╨╕ ╨╝╤Г╤Ж╨╕╤Г╤Б ╨╗╨░╨▒╨╕╤В╤Г╤А ╨╕╨┤. ╨Х╤В ╤Е╨╕╤Б ╨╜╨╛╨╜╤Г╨╝╨╡╤Б ╨╜╨╛╨╗╤Г╨╕╤Б╤Б╨╡ ╨┤╨╕╨│╨╜╨╕╤Б╤Б╨╕╨╝.",
        vec!["╨Ы╨╛╤А╨╡╨╝", " ", "╨╕╨┐╤Б╤Г╨╝", " ", "╨┤╨╛╨╗╨╛╤А", " ", "╤Б╨╕╤В", " ", "╨░╨╝╨╡╤В", ",", " ", "╨┐╨╡╤А", " ", "╤Ж╨╗╨╕╤В╨░", " ", "╨┐╨╛╤Б╤Б╨╕╤В", " ", "╨╡╤Е", ",", " ", "╨░╤В", " ", "╨╝╤Г╨╜╨╡╤А╨╡", " ", "╤Д╨░╨▒╤Г╨╗╨░╤Б", " ", "╨┐╨╡╤В╨╡╨╜╤В╨╕╤Г╨╝", " ", "╤Б╨╕╤В", ".", " ", "╨Ш╤Г╤Б", " ", "╤Ж╤Г", " ", "╤Ж╨╕╨▒╨╛", " ", "╤Б╨░╨┐╨╡╤А╨╡╤В", " ", "╤Б╤Ж╤А╨╕╨┐╤Б╨╡╤А╨╕╤В", ",", " ", "╨╜╨╡╤Ж", " ", "╨▓╨╕╤Б╨╕", " ", "╨╝╤Г╤Ж╨╕╤Г╤Б", " ", "╨╗╨░╨▒╨╕╤В╤Г╤А", " ", "╨╕╨┤", ".", " ", "╨Х╤В", " ", "╤Е╨╕╤Б", " ", "╨╜╨╛╨╜╤Г╨╝╨╡╤Б", " ", "╨╜╨╛╨╗╤Г╨╕╤Б╤Б╨╡", " ", "╨┤╨╕╨│╨╜╨╕╤Б╤Б╨╕╨╝", "."]
        ; "cyrillic script"
    )]
    #[test_case(
        "ъ╡нып╝ъ▓╜ьаЬьЭШ ы░ЬьаДьЭД ьЬДэХЬ ьдСьЪФьаХь▒ЕьЭШ ьИШыж╜ьЧР ъ┤АэХШьЧм ыМАэЖ╡ыа╣ьЭШ ьЮРым╕ьЧР ьЭСэХШъ╕░ ьЬДэХШьЧм ъ╡нып╝ъ▓╜ьаЬьЮРым╕эЪМьЭШые╝ ыСШ ьИШ ьЮИыЛд.",
        vec!["ъ╡нып╝ъ▓╜ьаЬьЭШ", " ", "ы░ЬьаДьЭД", " ", "ьЬДэХЬ", " ", "ьдСьЪФьаХь▒ЕьЭШ", " ", "ьИШыж╜ьЧР", " ", "ъ┤АэХШьЧм", " ", "ыМАэЖ╡ыа╣ьЭШ", " ", "ьЮРым╕ьЧР", " ", "ьЭСэХШъ╕░", " ", "ьЬДэХШьЧм", " ", "ъ╡нып╝ъ▓╜ьаЬьЮРым╕эЪМьЭШые╝", " ", "ыСШ", " ", "ьИШ", " ", "ьЮИыЛд", "."]
        ; "korean script"
    )]
    #[test_case(
        "╬Ы╬┐╧Б╬╡╬╝ ╬╣╧А╧Г╬╕╬╝ ╬┤╬┐╬╗╬┐╧Б ╧Г╬╣╧Д ╬▒╬╝╬╡╧Д, ╬╝╬╡╬╣ ╬╣╬┤ ╬╜╬┐v╬╕╬╝ ╧Ж╬▒╬▓╬╡╬╗╬╗╬▒╧Г ╧А╬╡╧Д╬╡╬╜╧Д╬╣╬╕╬╝ v╬╡╬╗ ╬╜╬╡, ╬▒╧Д ╬╜╬╣╧Г╬╗ ╧Г╬┐╬╜╬╡╧Д ╬┐╧А╬┐╧Б╧Д╬╡╧Б╬╡ ╬╡╬╕╬╝. ╬С╬╗╬╣╬╣ ╬┤╬┐c╧Д╬╕╧Г ╬╝╬╡╬╣ ╬╣╬┤, ╬╜╬┐ ╬▒╬╕╧Д╬╡╬╝ ╬▒╬╕╬┤╬╣╧Б╬╡ ╬╣╬╜╧Д╬╡╧Б╬╡╧Г╧Г╬╡╧Д ╬╝╬╡╬╗, ╬┤╬┐c╬╡╬╜╬┤╬╣ c╬┐╬╝╬╝╬╕╬╜╬╡ ╬┐╧А╬┐╧Б╧Д╬╡╬▒╧Д ╧Д╬╡ c╬╕╬╝.",
        vec!["╬Ы╬┐╧Б╬╡╬╝", " ", "╬╣╧А╧Г╬╕╬╝", " ", "╬┤╬┐╬╗╬┐╧Б", " ", "╧Г╬╣╧Д", " ", "╬▒╬╝╬╡╧Д", ",", " ", "╬╝╬╡╬╣", " ", "╬╣╬┤", " ", "╬╜╬┐v╬╕╬╝", " ", "╧Ж╬▒╬▓╬╡╬╗╬╗╬▒╧Г", " ", "╧А╬╡╧Д╬╡╬╜╧Д╬╣╬╕╬╝", " ", "v╬╡╬╗", " ", "╬╜╬╡", ",", " ", "╬▒╧Д", " ", "╬╜╬╣╧Г╬╗", " ", "╧Г╬┐╬╜╬╡╧Д", " ", "╬┐╧А╬┐╧Б╧Д╬╡╧Б╬╡", " ", "╬╡╬╕╬╝", ".", " ", "╬С╬╗╬╣╬╣", " ", "╬┤╬┐c╧Д╬╕╧Г", " ", "╬╝╬╡╬╣", " ", "╬╣╬┤", ",", " ", "╬╜╬┐", " ", "╬▒╬╕╧Д╬╡╬╝", " ", "╬▒╬╕╬┤╬╣╧Б╬╡", " ", "╬╣╬╜╧Д╬╡╧Б╬╡╧Г╧Г╬╡╧Д", " ", "╬╝╬╡╬╗", ",", " ", "╬┤╬┐c╬╡╬╜╬┤╬╣", " ", "c╬┐╬╝╬╝╬╕╬╜╬╡", " ", "╬┐╧А╬┐╧Б╧Д╬╡╬▒╧Д", " ", "╧Д╬╡", " ", "c╬╕╬╝", "."]
        ; "greek script"
    )]
    // TODO: check japanese script - why "уВ╗уГауГм" is not split?
    #[test_case(
        "цЧЕуГнф║мщЭТхИйуВ╗уГауГмх╝▒цФ╣уГХуГиуВ╣ц│вх║ЬуБЛуБ░уБ╝цДПщАБуБзуБ╝шк┐цО▓хпЯуБЯуВ╣цЧеше┐щЗНуВ▒уВвуГКф╜ПцйЛуГжуГауГЯуВпщаЖх╛ЕуБ╡уБЛуВУуБ╝ф║║хеиш▓пщПбуБЩуБ│уБЭуАВ",
        vec!["цЧЕ", "уГн", "ф║м", "щЭТ", "хИй", "уВ╗уГауГм", "х╝▒", "цФ╣", "уГХуГиуВ╣", "ц│в", "х║Ь", "уБЛ", "уБ░", "уБ╝", "цДП", "щАБ", "уБз", "уБ╝", "шк┐", "цО▓", "хпЯ", "уБЯ", "уВ╣", "цЧе", "ше┐", "щЗН", "уВ▒уВвуГК", "ф╜П", "цйЛ", "уГжуГауГЯуВп", "щаЖ", "х╛Е", "уБ╡", "уБЛ", "уВУ", "уБ╝", "ф║║", "хеи", "ш▓п", "щПб", "уБЩ", "уБ│", "уБЭ", "уАВ"]
        ; "japanese script"
    )]
    #[test_case(
        "хБ┤ч╡МцДПш▓мхо╢цЦ╣хо╢щЦЙшиОх║ЧцЪЦшВ▓чФ░х║Бш╝Йчд╛ш╗вч╖ЪхоЗуАВх╛ЧхРЫцЦ░шбУц▓╗ц╕йцКЧц╖╗ф╗гшй▒шАГцМпцКХхУбцо┤хдзщЧШхМЧшгБуАВхУБщЦУшнШщГицбИф╗гхнжхЗ░хЗжц╕Иц║Цф╕Цф╕АцИ╕хИ╗ц│ХхИЖуАВцВ╝ц╕мц╕ИшлПшиИщгпхИйхоЙхЗ╢цЦнчРЖш│Зц▓вхРМх▓йщЭвцЦЗшкНщЭйуАВхЖЕшнжца╝хМЦхЖНшЦмцЦ╣ф╣ЕхМЦф╜УцХЩх╛бц▒║цХ░шйншК╕х╛ЧчнЖф╗гуАВ",
        vec!["хБ┤", "ч╡М", "цДП", "ш▓м", "хо╢", "цЦ╣", "хо╢", "щЦЙ", "шиО", "х║Ч", "цЪЦ", "шВ▓", "чФ░", "х║Б", "ш╝Й", "чд╛", "ш╗в", "ч╖Ъ", "хоЗ", "уАВ", "х╛Ч", "хРЫ", "цЦ░", "шбУ", "ц▓╗", "ц╕й", "цКЧ", "ц╖╗", "ф╗г", "шй▒", "шАГ", "цМп", "цКХ", "хУб", "цо┤", "хдз", "щЧШ", "хМЧ", "шгБ", "уАВ", "хУБ", "щЦУ", "шнШ", "щГи", "цбИ", "ф╗г", "хнж", "хЗ░", "хЗж", "ц╕И", "ц║Ц", "ф╕Ц", "ф╕А", "цИ╕", "хИ╗", "ц│Х", "хИЖ", "уАВ", "цВ╝", "ц╕м", "ц╕И", "шлП", "шиИ", "щгп", "хИй", "хоЙ", "хЗ╢", "цЦн", "чРЖ", "ш│З", "ц▓в", "хРМ", "х▓й", "щЭв", "цЦЗ", "шкН", "щЭй", "уАВ", "хЖЕ", "шнж", "ца╝", "хМЦ", "хЖН", "шЦм", "цЦ╣", "ф╣Е", "хМЦ", "ф╜У", "цХЩ", "х╛б", "ц▒║", "цХ░", "шйн", "шК╕", "х╛Ч", "чнЖ", "ф╗г", "уАВ"]
        ; "chinese script"
    )]
    #[test_case(
        "рдкрдврд╛рдП рд╣рд┐рдВрджреА рд░рд╣рд╛рд░реБрдк рдЕрдиреБрд╡рд╛рдж рдХрд╛рд░реНрдпрд▓рдп рдореБрдЦреНрдп рд╕рдВрд╕реНрдерд╛ рд╕реЛреЮрддрд╡реЗрд░ рдирд┐рд░рдкреЗрдХреНрд╖ рдЙрдирдХрд╛ рдЖрдкрдХреЗ рдмрд╛рдЯрддреЗ рдЖрд╢рд╛рдЖрдкрд╕ рдореБрдЦреНрдпрддрд╣ рдЙрд╢рдХреА рдХрд░рддрд╛ред рд╢реБрд░реБрдЖрдд рд╕рдВрд╕реНрдерд╛ рдХреБрд╢рд▓рддрд╛ рдореЗрдВрднрдЯреГ рдЕрдиреБрд╡рд╛рдж рдЧрдПрдЖрдк рд╡рд┐рд╢реЗрд╖ рд╕рдХрддреЗ рдкрд░рд┐рднрд╛рд╖рд┐рдд рд▓рд╛рднрд╛рдиреНрд╡рд┐рдд рдкреНрд░рддрд┐ рджреЗрдХрд░ рд╕рдордЬрддреЗ рджрд┐рд╢рд╛рдореЗ рдкреНрд░рд╛рдкреНрдд рдЬреИрд╕реЗ рд╡рд░реНрдгрди рд╕рдВрд╕реНрдерд╛рди рдирд┐рд░реНрдорд╛рддрд╛ рдкреНрд░рд╡реНрд░реБрддрд┐ рднрд╛рддрд┐ рдЪреБрдирдиреЗ рдЙрдкрд▓рдмреНрдз рдмреЗрдВрдЧрд▓реВрд░ рдЕрд░реНрдердкреБрд░реНрдг",
        vec!["рдкрдврд╛рдП", " ", "рд╣рд┐рдВрджреА", " ", "рд░рд╣рд╛рд░реБрдк", " ", "рдЕрдиреБрд╡рд╛рдж", " ", "рдХрд╛рд░реНрдпрд▓рдп", " ", "рдореБрдЦреНрдп", " ", "рд╕рдВрд╕реНрдерд╛", " ", "рд╕реЛреЮрддрд╡реЗрд░", " ", "рдирд┐рд░рдкреЗрдХреНрд╖", " ", "рдЙрдирдХрд╛", " ", "рдЖрдкрдХреЗ", " ", "рдмрд╛рдЯрддреЗ", " ", "рдЖрд╢рд╛рдЖрдкрд╕", " ", "рдореБрдЦреНрдпрддрд╣", " ", "рдЙрд╢рдХреА", " ", "рдХрд░рддрд╛", "ред", " ", "рд╢реБрд░реБрдЖрдд", " ", "рд╕рдВрд╕реНрдерд╛", " ", "рдХреБрд╢рд▓рддрд╛", " ", "рдореЗрдВрднрдЯреГ", " ", "рдЕрдиреБрд╡рд╛рдж", " ", "рдЧрдПрдЖрдк", " ", "рд╡рд┐рд╢реЗрд╖", " ", "рд╕рдХрддреЗ", " ", "рдкрд░рд┐рднрд╛рд╖рд┐рдд", " ", "рд▓рд╛рднрд╛рдиреНрд╡рд┐рдд", " ", "рдкреНрд░рддрд┐", " ", "рджреЗрдХрд░", " ", "рд╕рдордЬрддреЗ", " ", "рджрд┐рд╢рд╛рдореЗ", " ", "рдкреНрд░рд╛рдкреНрдд", " ", "рдЬреИрд╕реЗ", " ", "рд╡рд░реНрдгрди", " ", "рд╕рдВрд╕реНрдерд╛рди", " ", "рдирд┐рд░реНрдорд╛рддрд╛", " ", "рдкреНрд░рд╡реНрд░реБрддрд┐", " ", "рднрд╛рддрд┐", " ", "рдЪреБрдирдиреЗ", " ", "рдЙрдкрд▓рдмреНрдз", " ", "рдмреЗрдВрдЧрд▓реВрд░", " ", "рдЕрд░реНрдердкреБрд░реНрдг"]
        ; "hindi script"
    )]
    #[test_case(
        "╒м╒╕╒╝╒е╒┤ ╒л╒║╒╜╒╕╓В╒┤ ╒д╒╕╒м╒╕╒╝ ╒╜╒л╒й ╒б╒┤╒е╒й, ╒м╒б╒в╒╕╒╝╒е ╒┤╒╕╒д╒е╒╝╒б╒й╒л╒╕╓В╒╜ ╒е╒й ╒░╒б╒╜, ╒║╒е╒╝ ╒╕╒┤╒╢╒л╒╜ ╒м╒б╒й╒л╒╢╒е ╒д╒л╒╜╒║╒╕╓В╒й╒б╒й╒л╒╕╒╢╒л ╒б╒й, ╒╛╒л╒╜ ╓Ж╒е╒╕╓В╒г╒б╒л╒й ╒о╒л╒╛╒л╒в╒╕╓В╒╜ ╒е╒н. ╒╛╒л╒╛╒е╒╢╒д╒╕╓В╒┤ ╒м╒б╒в╒╕╒╝╒б╒┤╒╕╓В╒╜ ╒е╒м╒б╒в╒╕╒╝╒б╒╝╒е╒й ╒╢╒б╒┤ ╒л╒╢.",
        vec!["╒м╒╕╒╝╒е╒┤", " ", "╒л╒║╒╜╒╕╓В╒┤", " ", "╒д╒╕╒м╒╕╒╝", " ", "╒╜╒л╒й", " ", "╒б╒┤╒е╒й", ",", " ", "╒м╒б╒в╒╕╒╝╒е", " ", "╒┤╒╕╒д╒е╒╝╒б╒й╒л╒╕╓В╒╜", " ", "╒е╒й", " ", "╒░╒б╒╜", ",", " ", "╒║╒е╒╝", " ", "╒╕╒┤╒╢╒л╒╜", " ", "╒м╒б╒й╒л╒╢╒е", " ", "╒д╒л╒╜╒║╒╕╓В╒й╒б╒й╒л╒╕╒╢╒л", " ", "╒б╒й", ",", " ", "╒╛╒л╒╜", " ", "╓Ж╒е╒╕╓В╒г╒б╒л╒й", " ", "╒о╒л╒╛╒л╒в╒╕╓В╒╜", " ", "╒е╒н", ".", " ", "╒╛╒л╒╛╒е╒╢╒д╒╕╓В╒┤", " ", "╒м╒б╒в╒╕╒╝╒б╒┤╒╕╓В╒╜", " ", "╒е╒м╒б╒в╒╕╒╝╒б╒╝╒е╒й", " ", "╒╢╒б╒┤", " ", "╒л╒╢", "."]
        ; "armenian script"
    )]
    #[test_case(
        "╪║┘К┘Ж┘К╪з ┘И╪з╪│╪к┘Е╪▒ ╪з┘Д╪╣╪╡╪и╪й ╪╢╪▒╪и ┘В╪п. ┘И╪и╪з╪б╪к ╪з┘Д╪г┘Е╪▒┘К┘Г┘К ╪з┘Д╪г┘И╪▒╪и┘К┘К┘Ж ┘З┘И ╪и┘З╪М, ┘З┘И ╪з┘Д╪╣╪з┘Д┘Е╪М ╪з┘Д╪л┘В┘К┘Д╪й ╪и╪з┘Д. ┘Е╪╣ ┘И╪з┘К╪▒┘Д┘Ж╪п╪з ╪з┘Д╪г┘И╪▒┘И╪и┘К┘С┘И┘Ж ┘Г╪з┘Ж, ┘В╪п ╪и╪н┘В ╪г╪│╪з╪и┘К╪╣ ╪з┘Д╪╣╪╕┘Е┘Й ┘И╪з╪╣╪к┘Д╪з╪б. ╪з┘Ж┘З ┘Г┘Д ┘И╪е┘В╪з┘Е╪й ╪з┘Д┘Е┘И╪з╪п.",
        vec!["╪║┘К┘Ж┘К╪з", " ", "┘И╪з╪│╪к┘Е╪▒", " ", "╪з┘Д╪╣╪╡╪и╪й", " ", "╪╢╪▒╪и", " ", "┘В╪п", ".", " ", "┘И╪и╪з╪б╪к", " ", "╪з┘Д╪г┘Е╪▒┘К┘Г┘К", " ", "╪з┘Д╪г┘И╪▒╪и┘К┘К┘Ж", " ", "┘З┘И", " ", "╪и┘З", "╪М", ",", " ", "┘З┘И", " ", "╪з┘Д╪╣╪з┘Д┘Е", "╪М", " ", "╪з┘Д╪л┘В┘К┘Д╪й", " ", "╪и╪з┘Д", ".", " ", "┘Е╪╣", " ", "┘И╪з┘К╪▒┘Д┘Ж╪п╪з", " ", "╪з┘Д╪г┘И╪▒┘И╪и┘К┘С┘И┘Ж", " ", "┘Г╪з┘Ж", ",", " ", "┘В╪п", " ", "╪и╪н┘В", " ", "╪г╪│╪з╪и┘К╪╣", " ", "╪з┘Д╪╣╪╕┘Е┘Й", " ", "┘И╪з╪╣╪к┘Д╪з╪б", ".", " ", "╪з┘Ж┘З", " ", "┘Г┘Д", " ", "┘И╪е┘В╪з┘Е╪й", " ", "╪з┘Д┘Е┘И╪з╪п", "."]
        ; "arabic script"
    )]
    #[test_case(
        "╫Ы╫У╫Щ ╫Щ╫б╫Х╫У ╫Ю╫Х╫а╫Ч╫Щ╫Э ╫Ю╫Х╫в╫Ю╫У╫Щ╫Э ╫й╫Ь, ╫У╫к ╫У╫д╫Щ╫Э ╫Ю╫Р╫Ю╫и╫й╫Щ╫Ч╫Ф╫ж╫д╫Ф ╫Ц╫Р╫к. ╫Р╫к╫Ф ╫У╫к ╫й╫Х╫а╫Ф ╫Ы╫Ь╫й╫Ф╫Х, ╫Т╫Э ╫Р╫Ч╫и ╫Ь╫Щ╫Х╫Э ╫С╫й╫д╫Х╫к, ╫Р╫Х ╫а╫Щ╫Х╫Х╫Ш ╫д╫Х╫Ь╫а╫Щ╫к ╫Ь╫Ч╫Щ╫С╫Х╫и ╫Р╫и╫е. ╫Х╫Щ╫й ╫С╫з╫Ь╫Х╫к ╫Х╫Р╫Ю╫а╫Х╫к ╫Р╫Щ╫и╫Х╫в╫Щ╫Э ╫Р╫Х, ╫Р╫Ь ╫Р╫Щ╫а╫Х ╫Ы╫Ь╫Ы╫Ь╫Ф ╫й╫к╫Щ.",
        vec!["╫Ы╫У╫Щ", " ", "╫Щ╫б╫Х╫У", " ", "╫Ю╫Х╫а╫Ч╫Щ╫Э", " ", "╫Ю╫Х╫в╫Ю╫У╫Щ╫Э", " ", "╫й╫Ь", ",", " ", "╫У╫к", " ", "╫У╫д╫Щ╫Э", " ", "╫Ю╫Р╫Ю╫и╫й╫Щ╫Ч╫Ф╫ж╫д╫Ф", " ", "╫Ц╫Р╫к", ".", " ", "╫Р╫к╫Ф", " ", "╫У╫к", " ", "╫й╫Х╫а╫Ф", " ", "╫Ы╫Ь╫й╫Ф╫Х", ",", " ", "╫Т╫Э", " ", "╫Р╫Ч╫и", " ", "╫Ь╫Щ╫Х╫Э", " ", "╫С╫й╫д╫Х╫к", ",", " ", "╫Р╫Х", " ", "╫а╫Щ╫Х╫Х╫Ш", " ", "╫д╫Х╫Ь╫а╫Щ╫к", " ", "╫Ь╫Ч╫Щ╫С╫Х╫и", " ", "╫Р╫и╫е", ".", " ", "╫Х╫Щ╫й", " ", "╫С╫з╫Ь╫Х╫к", " ", "╫Х╫Р╫Ю╫а╫Х╫к", " ", "╫Р╫Щ╫и╫Х╫в╫Щ╫Э", " ", "╫Р╫Х", ",", " ", "╫Р╫Ь", " ", "╫Р╫Щ╫а╫Х", " ", "╫Ы╫Ь╫Ы╫Ь╫Ф", " ", "╫й╫к╫Щ", "."]
        ; "hebrew script"
    )]
    fn test_multilingual_lorem_ipsum(input_text: &str, expected_tokens: Vec<&str>) {
        let doc = Doc::new(input_text);
        let expected_tokens = expected_tokens.iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
        assert_eq!(doc.tokens.len(), expected_tokens.len());
        assert_eq!(doc.tokens, expected_tokens);
    }

    #[test_case(vec!["Hello", ",", " ", "world", "!"] ; "latin script")]
    #[test_case(vec!["    ", "Some", "\t", "\t", "    ", "spaces"] ; "complicated spaces")]
    fn test_docs_from_tokens(tokens: Vec<&str>) {
        let doc = Doc::from_tokens(tokens.clone());
        let expected_tokens = tokens.iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
        assert_eq!(doc.tokens.len(), tokens.len());
        assert_eq!(doc.tokens, expected_tokens);
    }

    #[test_case("Hello, world!" ; "latin script")]
    #[test_case("    Some\t\t    spaces" ; "complicated spaces")]
    #[test_case("Hello\u{200A}world" ; "unicode spaces")]
    #[test_case("╨Ы╨╛╤А╨╡╨╝ ╨╕╨┐╤Б╤Г╨╝, ╤О╤Б╤В╨╛ ╨┤╨╕╤Ж╤В╨░╤Б ╨╡╨╕." ; "cyrillic script")]
    #[test_case("ф╕ЛхзРя╝МхБЪхЕТщЗЗуАВ" ; "chinese script")]
    #[test_case("┘В╪п ┘Б╪з╪к┘С╪и╪╣ ┘И╪е╪╣┘Д╪з┘Ж ╪н╪п┘Й. ┘Ж┘В╪╖╪й ╪│┘В┘И╪╖" ; "arabic script")]
    #[test_case(".!@#$%^&*()_+" ; "special characters")]
    fn test_to_string(text: &str) {
        let text_copy = text.to_string();
        let doc = Doc::new(text);
        assert_eq!(doc.to_string(), text_copy);
    }

    #[test_case(vec!["A", ",", " ", "B", "!"], false, 2 ; "only words")]
    #[test_case(vec!["A", ",", " ", "B", "!"], true, 4 ; "words with special chars")]
    fn test_word_tokens_count(tokens: Vec<&str>, include_special_char: bool, expected: usize) {
        let doc = Doc::from_tokens(tokens);
        assert_eq!(doc.get_word_tokens_count(include_special_char), expected);
    }

    #[test_case(vec!["A", "B", "C", "D"], false, vec![0, 1, 2, 3])]
    #[test_case(vec!["A", "B", "C", "D"], true, vec![0, 1, 2, 3])]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], false, vec![0, 3, 4])]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], true, vec![0, 1, 3, 4, 5])]
    fn test_get_word_indexes(tokens: Vec<&str>, include_special_char: bool, expected: Vec<usize>) {
        let mut doc = Doc::from_tokens(tokens);
        let word_tokens = doc.get_word_indexes(include_special_char, None);
        assert_eq!(word_tokens, expected);
    }

    #[test_case(vec!["A", "B", "C", "D"], false, vec!["A"], vec![1, 2, 3] ; "only words filter single word")]
    #[test_case(vec!["A", "B", "C", "D"], false, vec![], vec![0, 1, 2, 3] ; "only words filter empty")]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], true, vec!["A", "B"], vec![1, 4, 5] ; "words with special chars filter multiple")]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], false, vec!["A", "B"], vec![4] ; "filter existing")]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], false, vec!["C", "E", "R"], vec![0, 3] ; "filter not existing")]
    #[test_case(vec!["A", "B", "C", "D"], true, vec!["A", "B", "C", "D", "F"], vec![] ; "all words filter all")]
    fn test_get_word_indexes_without_stopwords(tokens: Vec<&str>, include_special_char: bool, stopwords: Vec<&str>, expected: Vec<usize>) {
        let stopwords = stopwords.iter().map(|&token| token.to_string()).collect::<HashSet<String>>();
        let mut doc = Doc::from_tokens(tokens);
        let word_tokens = doc.get_word_indexes(include_special_char, Some(&stopwords));
        assert_eq!(word_tokens, expected);
    }

    #[test_case("A B, C D", 0, 2, "B A, C D")]
    #[test_case("A B, C D", 0, 1, " AB, C D")]
    #[test_case("A B, C D", 2, 3, "A ,B C D")]
    #[test_case("A B, C D", 7, 1, "ADB, C  ")]
    fn test_swap_tokens_by_index(text: &str, idx_a: usize, idx_b: usize, expected: &str) {
        let mut doc = Doc::new(text);
        doc.swap_tokens_by_index(idx_a, idx_b);
        assert_eq!(doc.to_string(), expected);
    }
}
