use std::collections::HashSet;
use super::token::{Token, TokenType};
use unicode_segmentation::UnicodeSegmentation;


/// Doc struct holds content as a list of tokens.
/// TODO: Lazy loading of tokens while not requested.
pub struct Doc {
    pub tokens: Vec<Token>,
    pub num_changes: usize,
}

impl Doc {
    /// Create a new Doc from a string slice.
    /// Automatically tokenizes the text on word boundaries (words, spaces, and special symbols).
    /// See https://www.unicode.org/reports/tr29/#Word_Boundaries
    ///
    /// # Arguments
    /// * `text` - A string slice that holds the text to be tokenized.
    ///
    /// # Examples
    /// ```rust
    /// use fast_aug::text::doc::Doc;
    /// use fast_aug::text::token::Token;
    ///
    /// let doc = Doc::new("Hello,  world!");
    /// let expected_tokens = vec!["Hello", ",", "  ", "world", "!"].iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
    /// assert_eq!(doc.tokens, expected_tokens);
    /// ```
    pub fn new(text: &str) -> Self {
        let tokens = Doc::tokenize(text);
        Doc {
            tokens,
            num_changes: 0,
        }
    }

    /// Create a new Doc from a list of tokens.
    /// Select token type automatically.
    ///
    /// # Arguments
    /// * `tokens` - A vector of string slices that holds the tokens.
    ///
    /// # Examples
    /// ```rust
    /// use fast_aug::text::doc::Doc;
    /// use fast_aug::text::token::Token;
    ///
    /// let doc = Doc::from_tokens(vec!["Hello", ",", "  ", "world", "!"]);
    /// let expected_tokens = vec!["Hello", ",", "  ", "world", "!"].iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
    /// assert_eq!(doc.tokens, expected_tokens);
    /// ```
    pub fn from_tokens(tokens: Vec<&str>) -> Self {
        let tokens = tokens.iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
        Doc {
            tokens,
            num_changes: 0,
        }
    }

    /// Tokenize a string slice on word boundaries (words, spaces, and special symbols).
    /// Use "Unicode Standard Annex #29" https://www.unicode.org/reports/tr29/#Word_Boundaries
    fn tokenize(text: &str) -> Vec<Token> {
        UnicodeSegmentation::split_word_bounds(text)
            .map(Token::from_str)
            .collect()
    }

    pub fn len(&self) -> usize {
        self.tokens.len()
    }

    pub fn is_empty(&self) -> bool {
        self.tokens.is_empty()
    }

    /// Convert Doc to string
    pub fn to_string(&self) -> String {
        self.tokens.iter().map(|token| token.token().as_str()).collect::<Vec<&str>>().join("")
    }

    /// Calculate number of word tokens
    ///
    /// # Arguments
    /// * `include_special_char` - Include Special tokens in count
    pub fn get_word_tokens_count(&self, include_special_char: bool) -> usize {
        let mut count = 0;
        for token in self.tokens.iter() {
            let token_type = token.kind();
            match (token_type, include_special_char) {
                (TokenType::Word, _) => count += 1,
                (TokenType::Special, true) => count += 1,
                (_, _) => (),
            }
        }
        count
    }

    /// Get only WordTokens original indexes
    ///
    /// # Arguments
    /// * `include_special_char` - Include Special tokens in count
    /// * `stopwords` - A HashSet of stopwords to be skipped
    pub fn get_word_indexes(&mut self, include_special_char: bool, stopwords: Option<&HashSet<String>>) -> Vec<usize> {
        let mut word_indexes = Vec::with_capacity(self.tokens.len());

        for (idx, token) in self.tokens.iter().enumerate() {
            let token_type = token.kind();
            match (token_type, include_special_char) {
                (TokenType::Word, _) => {
                    if let Some(stopwords) = stopwords {
                        if stopwords.contains(token.token()) {
                            continue;
                        }
                    }
                    word_indexes.push(idx);
                },
                (TokenType::Special, true) => word_indexes.push(idx),
                (_, _) => (),
            }
        }

        word_indexes
    }

    /// Swap two tokens in Doc - in-place
    ///
    /// # Arguments
    /// * `idx_a` - Index of first token
    /// * `idx_b` - Index of second token
    pub fn swap_tokens_by_index(&mut self, idx_a: usize, idx_b: usize) {
        self.tokens.swap(idx_a, idx_b);
    }
}


#[cfg(test)]
mod tests {
    use test_case::test_case;
    use super::*;

    #[test_case("Hello, world!", vec!["Hello", ",", " ", "world", "!"] ; "basic latin script")]
    #[test_case("    Some\t\t    spaces", vec!["    ", "Some", "\t", "\t", "    ", "spaces"] ; "complicated spaces")]
    #[test_case("Hello\u{200A}world", vec!["Hello", "\u{200A}", "world"] ; "unicode spaces")]
    #[test_case(".!@#$%^&*()_+", vec![".", "!", "@", "#", "$", "%", "^", "&", "*", "(", ")", "_", "+"] ; "special characters")]
    #[test_case("ğŸ˜›ğŸ™ˆğŸ˜†", vec!["ğŸ˜›", "ğŸ™ˆ", "ğŸ˜†"] ; "emoji")]
    #[test_case(":clown_face: :see_no_evil: :laughing:", vec![":", "clown_face", ":", " ", ":", "see_no_evil", ":", " ", ":", "laughing", ":"] ; "emoji aliases")]
    fn test_docs_spesial_cases(input_text: &str, expected_tokens: Vec<&str>) {
        let doc = Doc::new(input_text);
        let expected_tokens = expected_tokens.iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
        assert_eq!(doc.tokens.len(), expected_tokens.len());
        assert_eq!(doc.tokens, expected_tokens);
    }

    // from https://generator.lorem-ipsum.info/
    #[test_case(
        "Lorem ipsum dolor sit amet, soluta commune ponderum mea ad, te qui erant vulputate, ne sed noster verear. No illud abhorreant vel, quo no amet liber tantas.",
        vec!["Lorem", " ", "ipsum", " ", "dolor", " ", "sit", " ", "amet", ",", " ", "soluta", " ", "commune", " ", "ponderum", " ", "mea", " ", "ad", ",", " ", "te", " ", "qui", " ", "erant", " ", "vulputate", ",", " ", "ne", " ", "sed", " ", "noster", " ", "verear", ".", " ", "No", " ", "illud", " ", "abhorreant", " ", "vel", ",", " ", "quo", " ", "no", " ", "amet", " ", "liber", " ", "tantas", "."]
        ; "latin script"
    )]
    #[test_case(
        "Ğ›Ğ¾Ñ€ĞµĞ¼ Ğ¸Ğ¿ÑÑƒĞ¼ Ğ´Ğ¾Ğ»Ğ¾Ñ€ ÑĞ¸Ñ‚ Ğ°Ğ¼ĞµÑ‚, Ğ¿ĞµÑ€ Ñ†Ğ»Ğ¸Ñ‚Ğ° Ğ¿Ğ¾ÑÑĞ¸Ñ‚ ĞµÑ…, Ğ°Ñ‚ Ğ¼ÑƒĞ½ĞµÑ€Ğµ Ñ„Ğ°Ğ±ÑƒĞ»Ğ°Ñ Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ¸ÑƒĞ¼ ÑĞ¸Ñ‚. Ğ˜ÑƒÑ Ñ†Ñƒ Ñ†Ğ¸Ğ±Ğ¾ ÑĞ°Ğ¿ĞµÑ€ĞµÑ‚ ÑÑ†Ñ€Ğ¸Ğ¿ÑĞµÑ€Ğ¸Ñ‚, Ğ½ĞµÑ† Ğ²Ğ¸ÑĞ¸ Ğ¼ÑƒÑ†Ğ¸ÑƒÑ Ğ»Ğ°Ğ±Ğ¸Ñ‚ÑƒÑ€ Ğ¸Ğ´. Ğ•Ñ‚ Ñ…Ğ¸Ñ Ğ½Ğ¾Ğ½ÑƒĞ¼ĞµÑ Ğ½Ğ¾Ğ»ÑƒĞ¸ÑÑĞµ Ğ´Ğ¸Ğ³Ğ½Ğ¸ÑÑĞ¸Ğ¼.",
        vec!["Ğ›Ğ¾Ñ€ĞµĞ¼", " ", "Ğ¸Ğ¿ÑÑƒĞ¼", " ", "Ğ´Ğ¾Ğ»Ğ¾Ñ€", " ", "ÑĞ¸Ñ‚", " ", "Ğ°Ğ¼ĞµÑ‚", ",", " ", "Ğ¿ĞµÑ€", " ", "Ñ†Ğ»Ğ¸Ñ‚Ğ°", " ", "Ğ¿Ğ¾ÑÑĞ¸Ñ‚", " ", "ĞµÑ…", ",", " ", "Ğ°Ñ‚", " ", "Ğ¼ÑƒĞ½ĞµÑ€Ğµ", " ", "Ñ„Ğ°Ğ±ÑƒĞ»Ğ°Ñ", " ", "Ğ¿ĞµÑ‚ĞµĞ½Ñ‚Ğ¸ÑƒĞ¼", " ", "ÑĞ¸Ñ‚", ".", " ", "Ğ˜ÑƒÑ", " ", "Ñ†Ñƒ", " ", "Ñ†Ğ¸Ğ±Ğ¾", " ", "ÑĞ°Ğ¿ĞµÑ€ĞµÑ‚", " ", "ÑÑ†Ñ€Ğ¸Ğ¿ÑĞµÑ€Ğ¸Ñ‚", ",", " ", "Ğ½ĞµÑ†", " ", "Ğ²Ğ¸ÑĞ¸", " ", "Ğ¼ÑƒÑ†Ğ¸ÑƒÑ", " ", "Ğ»Ğ°Ğ±Ğ¸Ñ‚ÑƒÑ€", " ", "Ğ¸Ğ´", ".", " ", "Ğ•Ñ‚", " ", "Ñ…Ğ¸Ñ", " ", "Ğ½Ğ¾Ğ½ÑƒĞ¼ĞµÑ", " ", "Ğ½Ğ¾Ğ»ÑƒĞ¸ÑÑĞµ", " ", "Ğ´Ğ¸Ğ³Ğ½Ğ¸ÑÑĞ¸Ğ¼", "."]
        ; "cyrillic script"
    )]
    #[test_case(
        "êµ­ë¯¼ê²½ì œì˜ ë°œì „ì„ ìœ„í•œ ì¤‘ìš”ì •ì±…ì˜ ìˆ˜ë¦½ì— ê´€í•˜ì—¬ ëŒ€í†µë ¹ì˜ ìë¬¸ì— ì‘í•˜ê¸° ìœ„í•˜ì—¬ êµ­ë¯¼ê²½ì œìë¬¸íšŒì˜ë¥¼ ë‘˜ ìˆ˜ ìˆë‹¤.",
        vec!["êµ­ë¯¼ê²½ì œì˜", " ", "ë°œì „ì„", " ", "ìœ„í•œ", " ", "ì¤‘ìš”ì •ì±…ì˜", " ", "ìˆ˜ë¦½ì—", " ", "ê´€í•˜ì—¬", " ", "ëŒ€í†µë ¹ì˜", " ", "ìë¬¸ì—", " ", "ì‘í•˜ê¸°", " ", "ìœ„í•˜ì—¬", " ", "êµ­ë¯¼ê²½ì œìë¬¸íšŒì˜ë¥¼", " ", "ë‘˜", " ", "ìˆ˜", " ", "ìˆë‹¤", "."]
        ; "korean script"
    )]
    #[test_case(
        "Î›Î¿ÏÎµÎ¼ Î¹Ï€ÏƒÎ¸Î¼ Î´Î¿Î»Î¿Ï ÏƒÎ¹Ï„ Î±Î¼ÎµÏ„, Î¼ÎµÎ¹ Î¹Î´ Î½Î¿vÎ¸Î¼ Ï†Î±Î²ÎµÎ»Î»Î±Ïƒ Ï€ÎµÏ„ÎµÎ½Ï„Î¹Î¸Î¼ vÎµÎ» Î½Îµ, Î±Ï„ Î½Î¹ÏƒÎ» ÏƒÎ¿Î½ÎµÏ„ Î¿Ï€Î¿ÏÏ„ÎµÏÎµ ÎµÎ¸Î¼. Î‘Î»Î¹Î¹ Î´Î¿cÏ„Î¸Ïƒ Î¼ÎµÎ¹ Î¹Î´, Î½Î¿ Î±Î¸Ï„ÎµÎ¼ Î±Î¸Î´Î¹ÏÎµ Î¹Î½Ï„ÎµÏÎµÏƒÏƒÎµÏ„ Î¼ÎµÎ», Î´Î¿cÎµÎ½Î´Î¹ cÎ¿Î¼Î¼Î¸Î½Îµ Î¿Ï€Î¿ÏÏ„ÎµÎ±Ï„ Ï„Îµ cÎ¸Î¼.",
        vec!["Î›Î¿ÏÎµÎ¼", " ", "Î¹Ï€ÏƒÎ¸Î¼", " ", "Î´Î¿Î»Î¿Ï", " ", "ÏƒÎ¹Ï„", " ", "Î±Î¼ÎµÏ„", ",", " ", "Î¼ÎµÎ¹", " ", "Î¹Î´", " ", "Î½Î¿vÎ¸Î¼", " ", "Ï†Î±Î²ÎµÎ»Î»Î±Ïƒ", " ", "Ï€ÎµÏ„ÎµÎ½Ï„Î¹Î¸Î¼", " ", "vÎµÎ»", " ", "Î½Îµ", ",", " ", "Î±Ï„", " ", "Î½Î¹ÏƒÎ»", " ", "ÏƒÎ¿Î½ÎµÏ„", " ", "Î¿Ï€Î¿ÏÏ„ÎµÏÎµ", " ", "ÎµÎ¸Î¼", ".", " ", "Î‘Î»Î¹Î¹", " ", "Î´Î¿cÏ„Î¸Ïƒ", " ", "Î¼ÎµÎ¹", " ", "Î¹Î´", ",", " ", "Î½Î¿", " ", "Î±Î¸Ï„ÎµÎ¼", " ", "Î±Î¸Î´Î¹ÏÎµ", " ", "Î¹Î½Ï„ÎµÏÎµÏƒÏƒÎµÏ„", " ", "Î¼ÎµÎ»", ",", " ", "Î´Î¿cÎµÎ½Î´Î¹", " ", "cÎ¿Î¼Î¼Î¸Î½Îµ", " ", "Î¿Ï€Î¿ÏÏ„ÎµÎ±Ï„", " ", "Ï„Îµ", " ", "cÎ¸Î¼", "."]
        ; "greek script"
    )]
    // TODO: check japanese script - why "ã‚»ãƒ ãƒ¬" is not split?
    #[test_case(
        "æ—…ãƒ­äº¬é’åˆ©ã‚»ãƒ ãƒ¬å¼±æ”¹ãƒ•ãƒ¨ã‚¹æ³¢åºœã‹ã°ã¼æ„é€ã§ã¼èª¿æ²å¯ŸãŸã‚¹æ—¥è¥¿é‡ã‚±ã‚¢ãƒŠä½æ©‹ãƒ¦ãƒ ãƒŸã‚¯é †å¾…ãµã‹ã‚“ã¼äººå¥¨è²¯é¡ã™ã³ãã€‚",
        vec!["æ—…", "ãƒ­", "äº¬", "é’", "åˆ©", "ã‚»ãƒ ãƒ¬", "å¼±", "æ”¹", "ãƒ•ãƒ¨ã‚¹", "æ³¢", "åºœ", "ã‹", "ã°", "ã¼", "æ„", "é€", "ã§", "ã¼", "èª¿", "æ²", "å¯Ÿ", "ãŸ", "ã‚¹", "æ—¥", "è¥¿", "é‡", "ã‚±ã‚¢ãƒŠ", "ä½", "æ©‹", "ãƒ¦ãƒ ãƒŸã‚¯", "é †", "å¾…", "ãµ", "ã‹", "ã‚“", "ã¼", "äºº", "å¥¨", "è²¯", "é¡", "ã™", "ã³", "ã", "ã€‚"]
        ; "japanese script"
    )]
    #[test_case(
        "å´çµŒæ„è²¬å®¶æ–¹å®¶é–‰è¨åº—æš–è‚²ç”°åºè¼‰ç¤¾è»¢ç·šå®‡ã€‚å¾—å›æ–°è¡“æ²»æ¸©æŠ—æ·»ä»£è©±è€ƒæŒ¯æŠ•å“¡æ®´å¤§é—˜åŒ—è£ã€‚å“é–“è­˜éƒ¨æ¡ˆä»£å­¦å‡°å‡¦æ¸ˆæº–ä¸–ä¸€æˆ¸åˆ»æ³•åˆ†ã€‚æ‚¼æ¸¬æ¸ˆè«è¨ˆé£¯åˆ©å®‰å‡¶æ–­ç†è³‡æ²¢åŒå²©é¢æ–‡èªé©ã€‚å†…è­¦æ ¼åŒ–å†è–¬æ–¹ä¹…åŒ–ä½“æ•™å¾¡æ±ºæ•°è©­èŠ¸å¾—ç­†ä»£ã€‚",
        vec!["å´", "çµŒ", "æ„", "è²¬", "å®¶", "æ–¹", "å®¶", "é–‰", "è¨", "åº—", "æš–", "è‚²", "ç”°", "åº", "è¼‰", "ç¤¾", "è»¢", "ç·š", "å®‡", "ã€‚", "å¾—", "å›", "æ–°", "è¡“", "æ²»", "æ¸©", "æŠ—", "æ·»", "ä»£", "è©±", "è€ƒ", "æŒ¯", "æŠ•", "å“¡", "æ®´", "å¤§", "é—˜", "åŒ—", "è£", "ã€‚", "å“", "é–“", "è­˜", "éƒ¨", "æ¡ˆ", "ä»£", "å­¦", "å‡°", "å‡¦", "æ¸ˆ", "æº–", "ä¸–", "ä¸€", "æˆ¸", "åˆ»", "æ³•", "åˆ†", "ã€‚", "æ‚¼", "æ¸¬", "æ¸ˆ", "è«", "è¨ˆ", "é£¯", "åˆ©", "å®‰", "å‡¶", "æ–­", "ç†", "è³‡", "æ²¢", "åŒ", "å²©", "é¢", "æ–‡", "èª", "é©", "ã€‚", "å†…", "è­¦", "æ ¼", "åŒ–", "å†", "è–¬", "æ–¹", "ä¹…", "åŒ–", "ä½“", "æ•™", "å¾¡", "æ±º", "æ•°", "è©­", "èŠ¸", "å¾—", "ç­†", "ä»£", "ã€‚"]
        ; "chinese script"
    )]
    #[test_case(
        "à¤ªà¤¢à¤¾à¤ à¤¹à¤¿à¤‚à¤¦à¥€ à¤°à¤¹à¤¾à¤°à¥à¤ª à¤…à¤¨à¥à¤µà¤¾à¤¦ à¤•à¤¾à¤°à¥à¤¯à¤²à¤¯ à¤®à¥à¤–à¥à¤¯ à¤¸à¤‚à¤¸à¥à¤¥à¤¾ à¤¸à¥‹à¥à¤¤à¤µà¥‡à¤° à¤¨à¤¿à¤°à¤ªà¥‡à¤•à¥à¤· à¤‰à¤¨à¤•à¤¾ à¤†à¤ªà¤•à¥‡ à¤¬à¤¾à¤Ÿà¤¤à¥‡ à¤†à¤¶à¤¾à¤†à¤ªà¤¸ à¤®à¥à¤–à¥à¤¯à¤¤à¤¹ à¤‰à¤¶à¤•à¥€ à¤•à¤°à¤¤à¤¾à¥¤ à¤¶à¥à¤°à¥à¤†à¤¤ à¤¸à¤‚à¤¸à¥à¤¥à¤¾ à¤•à¥à¤¶à¤²à¤¤à¤¾ à¤®à¥‡à¤‚à¤­à¤Ÿà¥ƒ à¤…à¤¨à¥à¤µà¤¾à¤¦ à¤—à¤à¤†à¤ª à¤µà¤¿à¤¶à¥‡à¤· à¤¸à¤•à¤¤à¥‡ à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤ à¤²à¤¾à¤­à¤¾à¤¨à¥à¤µà¤¿à¤¤ à¤ªà¥à¤°à¤¤à¤¿ à¤¦à¥‡à¤•à¤° à¤¸à¤®à¤œà¤¤à¥‡ à¤¦à¤¿à¤¶à¤¾à¤®à¥‡ à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤ à¤œà¥ˆà¤¸à¥‡ à¤µà¤°à¥à¤£à¤¨ à¤¸à¤‚à¤¸à¥à¤¥à¤¾à¤¨ à¤¨à¤¿à¤°à¥à¤®à¤¾à¤¤à¤¾ à¤ªà¥à¤°à¤µà¥à¤°à¥à¤¤à¤¿ à¤­à¤¾à¤¤à¤¿ à¤šà¥à¤¨à¤¨à¥‡ à¤‰à¤ªà¤²à¤¬à¥à¤§ à¤¬à¥‡à¤‚à¤—à¤²à¥‚à¤° à¤…à¤°à¥à¤¥à¤ªà¥à¤°à¥à¤£",
        vec!["à¤ªà¤¢à¤¾à¤", " ", "à¤¹à¤¿à¤‚à¤¦à¥€", " ", "à¤°à¤¹à¤¾à¤°à¥à¤ª", " ", "à¤…à¤¨à¥à¤µà¤¾à¤¦", " ", "à¤•à¤¾à¤°à¥à¤¯à¤²à¤¯", " ", "à¤®à¥à¤–à¥à¤¯", " ", "à¤¸à¤‚à¤¸à¥à¤¥à¤¾", " ", "à¤¸à¥‹à¥à¤¤à¤µà¥‡à¤°", " ", "à¤¨à¤¿à¤°à¤ªà¥‡à¤•à¥à¤·", " ", "à¤‰à¤¨à¤•à¤¾", " ", "à¤†à¤ªà¤•à¥‡", " ", "à¤¬à¤¾à¤Ÿà¤¤à¥‡", " ", "à¤†à¤¶à¤¾à¤†à¤ªà¤¸", " ", "à¤®à¥à¤–à¥à¤¯à¤¤à¤¹", " ", "à¤‰à¤¶à¤•à¥€", " ", "à¤•à¤°à¤¤à¤¾", "à¥¤", " ", "à¤¶à¥à¤°à¥à¤†à¤¤", " ", "à¤¸à¤‚à¤¸à¥à¤¥à¤¾", " ", "à¤•à¥à¤¶à¤²à¤¤à¤¾", " ", "à¤®à¥‡à¤‚à¤­à¤Ÿà¥ƒ", " ", "à¤…à¤¨à¥à¤µà¤¾à¤¦", " ", "à¤—à¤à¤†à¤ª", " ", "à¤µà¤¿à¤¶à¥‡à¤·", " ", "à¤¸à¤•à¤¤à¥‡", " ", "à¤ªà¤°à¤¿à¤­à¤¾à¤·à¤¿à¤¤", " ", "à¤²à¤¾à¤­à¤¾à¤¨à¥à¤µà¤¿à¤¤", " ", "à¤ªà¥à¤°à¤¤à¤¿", " ", "à¤¦à¥‡à¤•à¤°", " ", "à¤¸à¤®à¤œà¤¤à¥‡", " ", "à¤¦à¤¿à¤¶à¤¾à¤®à¥‡", " ", "à¤ªà¥à¤°à¤¾à¤ªà¥à¤¤", " ", "à¤œà¥ˆà¤¸à¥‡", " ", "à¤µà¤°à¥à¤£à¤¨", " ", "à¤¸à¤‚à¤¸à¥à¤¥à¤¾à¤¨", " ", "à¤¨à¤¿à¤°à¥à¤®à¤¾à¤¤à¤¾", " ", "à¤ªà¥à¤°à¤µà¥à¤°à¥à¤¤à¤¿", " ", "à¤­à¤¾à¤¤à¤¿", " ", "à¤šà¥à¤¨à¤¨à¥‡", " ", "à¤‰à¤ªà¤²à¤¬à¥à¤§", " ", "à¤¬à¥‡à¤‚à¤—à¤²à¥‚à¤°", " ", "à¤…à¤°à¥à¤¥à¤ªà¥à¤°à¥à¤£"]
        ; "hindi script"
    )]
    #[test_case(
        "Õ¬Õ¸Õ¼Õ¥Õ´ Õ«ÕºÕ½Õ¸Ö‚Õ´ Õ¤Õ¸Õ¬Õ¸Õ¼ Õ½Õ«Õ© Õ¡Õ´Õ¥Õ©, Õ¬Õ¡Õ¢Õ¸Õ¼Õ¥ Õ´Õ¸Õ¤Õ¥Õ¼Õ¡Õ©Õ«Õ¸Ö‚Õ½ Õ¥Õ© Õ°Õ¡Õ½, ÕºÕ¥Õ¼ Õ¸Õ´Õ¶Õ«Õ½ Õ¬Õ¡Õ©Õ«Õ¶Õ¥ Õ¤Õ«Õ½ÕºÕ¸Ö‚Õ©Õ¡Õ©Õ«Õ¸Õ¶Õ« Õ¡Õ©, Õ¾Õ«Õ½ Ö†Õ¥Õ¸Ö‚Õ£Õ¡Õ«Õ© Õ®Õ«Õ¾Õ«Õ¢Õ¸Ö‚Õ½ Õ¥Õ­. Õ¾Õ«Õ¾Õ¥Õ¶Õ¤Õ¸Ö‚Õ´ Õ¬Õ¡Õ¢Õ¸Õ¼Õ¡Õ´Õ¸Ö‚Õ½ Õ¥Õ¬Õ¡Õ¢Õ¸Õ¼Õ¡Õ¼Õ¥Õ© Õ¶Õ¡Õ´ Õ«Õ¶.",
        vec!["Õ¬Õ¸Õ¼Õ¥Õ´", " ", "Õ«ÕºÕ½Õ¸Ö‚Õ´", " ", "Õ¤Õ¸Õ¬Õ¸Õ¼", " ", "Õ½Õ«Õ©", " ", "Õ¡Õ´Õ¥Õ©", ",", " ", "Õ¬Õ¡Õ¢Õ¸Õ¼Õ¥", " ", "Õ´Õ¸Õ¤Õ¥Õ¼Õ¡Õ©Õ«Õ¸Ö‚Õ½", " ", "Õ¥Õ©", " ", "Õ°Õ¡Õ½", ",", " ", "ÕºÕ¥Õ¼", " ", "Õ¸Õ´Õ¶Õ«Õ½", " ", "Õ¬Õ¡Õ©Õ«Õ¶Õ¥", " ", "Õ¤Õ«Õ½ÕºÕ¸Ö‚Õ©Õ¡Õ©Õ«Õ¸Õ¶Õ«", " ", "Õ¡Õ©", ",", " ", "Õ¾Õ«Õ½", " ", "Ö†Õ¥Õ¸Ö‚Õ£Õ¡Õ«Õ©", " ", "Õ®Õ«Õ¾Õ«Õ¢Õ¸Ö‚Õ½", " ", "Õ¥Õ­", ".", " ", "Õ¾Õ«Õ¾Õ¥Õ¶Õ¤Õ¸Ö‚Õ´", " ", "Õ¬Õ¡Õ¢Õ¸Õ¼Õ¡Õ´Õ¸Ö‚Õ½", " ", "Õ¥Õ¬Õ¡Õ¢Õ¸Õ¼Õ¡Õ¼Õ¥Õ©", " ", "Õ¶Õ¡Õ´", " ", "Õ«Õ¶", "."]
        ; "armenian script"
    )]
    #[test_case(
        "ØºÙŠÙ†ÙŠØ§ ÙˆØ§Ø³ØªÙ…Ø± Ø§Ù„Ø¹ØµØ¨Ø© Ø¶Ø±Ø¨ Ù‚Ø¯. ÙˆØ¨Ø§Ø¡Øª Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠ Ø§Ù„Ø£ÙˆØ±Ø¨ÙŠÙŠÙ† Ù‡Ùˆ Ø¨Ù‡ØŒ, Ù‡Ùˆ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ Ø§Ù„Ø«Ù‚ÙŠÙ„Ø© Ø¨Ø§Ù„. Ù…Ø¹ ÙˆØ§ÙŠØ±Ù„Ù†Ø¯Ø§ Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠÙ‘ÙˆÙ† ÙƒØ§Ù†, Ù‚Ø¯ Ø¨Ø­Ù‚ Ø£Ø³Ø§Ø¨ÙŠØ¹ Ø§Ù„Ø¹Ø¸Ù…Ù‰ ÙˆØ§Ø¹ØªÙ„Ø§Ø¡. Ø§Ù†Ù‡ ÙƒÙ„ ÙˆØ¥Ù‚Ø§Ù…Ø© Ø§Ù„Ù…ÙˆØ§Ø¯.",
        vec!["ØºÙŠÙ†ÙŠØ§", " ", "ÙˆØ§Ø³ØªÙ…Ø±", " ", "Ø§Ù„Ø¹ØµØ¨Ø©", " ", "Ø¶Ø±Ø¨", " ", "Ù‚Ø¯", ".", " ", "ÙˆØ¨Ø§Ø¡Øª", " ", "Ø§Ù„Ø£Ù…Ø±ÙŠÙƒÙŠ", " ", "Ø§Ù„Ø£ÙˆØ±Ø¨ÙŠÙŠÙ†", " ", "Ù‡Ùˆ", " ", "Ø¨Ù‡", "ØŒ", ",", " ", "Ù‡Ùˆ", " ", "Ø§Ù„Ø¹Ø§Ù„Ù…", "ØŒ", " ", "Ø§Ù„Ø«Ù‚ÙŠÙ„Ø©", " ", "Ø¨Ø§Ù„", ".", " ", "Ù…Ø¹", " ", "ÙˆØ§ÙŠØ±Ù„Ù†Ø¯Ø§", " ", "Ø§Ù„Ø£ÙˆØ±ÙˆØ¨ÙŠÙ‘ÙˆÙ†", " ", "ÙƒØ§Ù†", ",", " ", "Ù‚Ø¯", " ", "Ø¨Ø­Ù‚", " ", "Ø£Ø³Ø§Ø¨ÙŠØ¹", " ", "Ø§Ù„Ø¹Ø¸Ù…Ù‰", " ", "ÙˆØ§Ø¹ØªÙ„Ø§Ø¡", ".", " ", "Ø§Ù†Ù‡", " ", "ÙƒÙ„", " ", "ÙˆØ¥Ù‚Ø§Ù…Ø©", " ", "Ø§Ù„Ù…ÙˆØ§Ø¯", "."]
        ; "arabic script"
    )]
    #[test_case(
        "×›×“×™ ×™×¡×•×“ ××•× ×—×™× ××•×¢××“×™× ×©×œ, ×“×ª ×“×¤×™× ××××¨×©×™×—×”×¦×¤×” ×–××ª. ××ª×” ×“×ª ×©×•× ×” ×›×œ×©×”×•, ×’× ××—×¨ ×œ×™×•× ×‘×©×¤×•×ª, ××• × ×™×•×•×˜ ×¤×•×œ× ×™×ª ×œ×—×™×‘×•×¨ ××¨×¥. ×•×™×© ×‘×§×œ×•×ª ×•××× ×•×ª ××™×¨×•×¢×™× ××•, ××œ ××™× ×• ×›×œ×›×œ×” ×©×ª×™.",
        vec!["×›×“×™", " ", "×™×¡×•×“", " ", "××•× ×—×™×", " ", "××•×¢××“×™×", " ", "×©×œ", ",", " ", "×“×ª", " ", "×“×¤×™×", " ", "××××¨×©×™×—×”×¦×¤×”", " ", "×–××ª", ".", " ", "××ª×”", " ", "×“×ª", " ", "×©×•× ×”", " ", "×›×œ×©×”×•", ",", " ", "×’×", " ", "××—×¨", " ", "×œ×™×•×", " ", "×‘×©×¤×•×ª", ",", " ", "××•", " ", "× ×™×•×•×˜", " ", "×¤×•×œ× ×™×ª", " ", "×œ×—×™×‘×•×¨", " ", "××¨×¥", ".", " ", "×•×™×©", " ", "×‘×§×œ×•×ª", " ", "×•××× ×•×ª", " ", "××™×¨×•×¢×™×", " ", "××•", ",", " ", "××œ", " ", "××™× ×•", " ", "×›×œ×›×œ×”", " ", "×©×ª×™", "."]
        ; "hebrew script"
    )]
    fn test_multilingual_lorem_ipsum(input_text: &str, expected_tokens: Vec<&str>) {
        let doc = Doc::new(input_text);
        let expected_tokens = expected_tokens.iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
        assert_eq!(doc.tokens.len(), expected_tokens.len());
        assert_eq!(doc.tokens, expected_tokens);
    }

    #[test_case(vec!["Hello", ",", " ", "world", "!"] ; "latin script")]
    #[test_case(vec!["    ", "Some", "\t", "\t", "    ", "spaces"] ; "complicated spaces")]
    fn test_docs_from_tokens(tokens: Vec<&str>) {
        let doc = Doc::from_tokens(tokens.clone());
        let expected_tokens = tokens.iter().map(|&token| Token::from_str(token)).collect::<Vec<Token>>();
        assert_eq!(doc.tokens.len(), tokens.len());
        assert_eq!(doc.tokens, expected_tokens);
    }

    #[test_case("Hello, world!" ; "latin script")]
    #[test_case("    Some\t\t    spaces" ; "complicated spaces")]
    #[test_case("Hello\u{200A}world" ; "unicode spaces")]
    #[test_case("Ğ›Ğ¾Ñ€ĞµĞ¼ Ğ¸Ğ¿ÑÑƒĞ¼, ÑÑÑ‚Ğ¾ Ğ´Ğ¸Ñ†Ñ‚Ğ°Ñ ĞµĞ¸." ; "cyrillic script")]
    #[test_case("ä¸‹å§ï¼Œåšå…’é‡‡ã€‚" ; "chinese script")]
    #[test_case("Ù‚Ø¯ ÙØ§ØªÙ‘Ø¨Ø¹ ÙˆØ¥Ø¹Ù„Ø§Ù† Ø­Ø¯Ù‰. Ù†Ù‚Ø·Ø© Ø³Ù‚ÙˆØ·" ; "arabic script")]
    #[test_case(".!@#$%^&*()_+" ; "special characters")]
    fn test_to_string(text: &str) {
        let text_copy = text.to_string();
        let doc = Doc::new(text);
        assert_eq!(doc.to_string(), text_copy);
    }

    #[test_case(vec!["A", ",", " ", "B", "!"], false, 2 ; "only words")]
    #[test_case(vec!["A", ",", " ", "B", "!"], true, 4 ; "words with special chars")]
    fn test_word_tokens_count(tokens: Vec<&str>, include_special_char: bool, expected: usize) {
        let doc = Doc::from_tokens(tokens);
        assert_eq!(doc.get_word_tokens_count(include_special_char), expected);
    }

    #[test_case(vec!["A", "B", "C", "D"], false, vec![0, 1, 2, 3])]
    #[test_case(vec!["A", "B", "C", "D"], true, vec![0, 1, 2, 3])]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], false, vec![0, 3, 4])]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], true, vec![0, 1, 3, 4, 5])]
    fn test_get_word_indexes(tokens: Vec<&str>, include_special_char: bool, expected: Vec<usize>) {
        let mut doc = Doc::from_tokens(tokens);
        let word_tokens = doc.get_word_indexes(include_special_char, None);
        assert_eq!(word_tokens, expected);
    }

    #[test_case(vec!["A", "B", "C", "D"], false, vec!["A"], vec![1, 2, 3] ; "only words filter single word")]
    #[test_case(vec!["A", "B", "C", "D"], false, vec![], vec![0, 1, 2, 3] ; "only words filter empty")]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], true, vec!["A", "B"], vec![1, 4, 5] ; "words with special chars filter multiple")]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], false, vec!["A", "B"], vec![4] ; "filter existing")]
    #[test_case(vec!["A", ",", " ", "B", "C", "!"], false, vec!["C", "E", "R"], vec![0, 3] ; "filter not existing")]
    #[test_case(vec!["A", "B", "C", "D"], true, vec!["A", "B", "C", "D", "F"], vec![] ; "all words filter all")]
    fn test_get_word_indexes_without_stopwords(tokens: Vec<&str>, include_special_char: bool, stopwords: Vec<&str>, expected: Vec<usize>) {
        let stopwords = stopwords.iter().map(|&token| token.to_string()).collect::<HashSet<String>>();
        let mut doc = Doc::from_tokens(tokens);
        let word_tokens = doc.get_word_indexes(include_special_char, Some(&stopwords));
        assert_eq!(word_tokens, expected);
    }

    #[test_case("A B, C D", 0, 2, "B A, C D")]
    #[test_case("A B, C D", 0, 1, " AB, C D")]
    #[test_case("A B, C D", 2, 3, "A ,B C D")]
    #[test_case("A B, C D", 7, 1, "ADB, C  ")]
    fn test_swap_tokens_by_index(text: &str, idx_a: usize, idx_b: usize, expected: &str) {
        let mut doc = Doc::new(text);
        doc.swap_tokens_by_index(idx_a, idx_b);
        assert_eq!(doc.to_string(), expected);
    }
}
